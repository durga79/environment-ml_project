\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}
\usepackage{hyperref}
\usepackage{balance}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Comparative Analysis of Machine Learning Methods for Environmental Impact Analysis: Air Quality, Climate Policy, and Water Quality Monitoring}

\author{
\IEEEauthorblockN{Gitte Swapna}
\IEEEauthorblockA{Student ID: 24202312\\
24202312@student.ncirl.ie}
\and
\IEEEauthorblockN{Hemasree Challa}
\IEEEauthorblockA{Student ID: 24267732\\
24267732@student.ncirl.ie}
\and
\IEEEauthorblockN{Student 3}
\IEEEauthorblockA{Student ID: XXXXXXXX\\
XXXXXXXX@student.ncirl.ie}
}

\maketitle

\begin{abstract}
Environmental monitoring faces growing challenges as data volumes increase from sensor networks, policy documents, and water quality assessments. This study explores how machine learning can address three specific problems: predicting air quality index categories from sensor measurements, classifying sentiment in climate policy texts, and determining water safety categories from water quality indicators. We tested six algorithms—Random Forest, XGBoost, Logistic Regression, Support Vector Machines, Decision Tree, and Gradient Boosting—following the CRISP-DM framework. Our air quality dataset contained 15,000 hourly measurements, the text dataset included 9,500 climate policy documents, and the water quality dataset comprised 12,000 monitoring records. For text analysis, we used TF-IDF to convert documents into numerical features, enabling classification into positive, neutral, or negative sentiment categories. The air quality and water quality models focused on identifying which environmental parameters most strongly predict their respective safety categories.

Our findings show Random Forest slightly outperformed XGBoost for air quality prediction, reaching 98.7\% accuracy with an F1-score of 0.987. Both Logistic Regression and SVM achieved 100\% accuracy on the sentiment classification task. For water quality, Gradient Boosting achieved 98.5\% accuracy, slightly outperforming Decision Tree at 97.8\%. We used SHAP and LIME to understand model decisions, revealing that PM2.5, PM10, and NO2 are the strongest predictors of air quality; specific vocabulary patterns drive sentiment classifications; and pH, dissolved oxygen, and turbidity are critical for water quality assessment.
\end{abstract}

\begin{IEEEkeywords}
Machine Learning, Environmental Analytics, Air Quality Prediction, Climate Policy Sentiment Analysis, Water Quality Classification, CRISP-DM, Text Classification, SHAP, LIME, Random Forest, XGBoost, Decision Tree, Gradient Boosting
\end{IEEEkeywords}

\section{Introduction}

Environmental issues like air pollution, climate change, and water contamination continue to worsen, creating massive amounts of data from monitoring stations, policy archives, and water quality assessments. Machine learning offers a way to make sense of this information, helping agencies track pollution levels, understand public response to climate policies, and ensure safe water supplies \cite{chapman2000crisp}. Our work tackles three specific problems that matter for public health and effective policy-making.

First, we wanted to see if we could automatically predict air quality categories from sensor readings. Monitoring stations produce thousands of measurements every hour, so manual checking isn't practical. Second, we explored whether we could classify the sentiment of climate policy documents—essentially understanding whether policies are viewed positively, negatively, or neutrally. Third, we investigated whether water quality indicators could reliably predict water safety categories, helping identify contamination risks early. Machine learning could enable real-time alerts and trend tracking across all three domains.

\subsection{Research Objectives}

We set out to accomplish several goals:

\begin{enumerate}
    \item Compare how well different machine learning approaches work on structured air quality data, unstructured text data, and water quality measurements.
    \item Test at least two algorithms on each dataset, following the CRISP-DM process.
    \item Use SHAP and LIME to understand what drives model predictions, making the results more useful for decision-makers.
    \item Figure out which factors matter most for predicting air quality, sentiment, and water safety.
    \item Offer practical guidance on how environmental agencies could use these methods.
\end{enumerate}

\subsection{Research Questions}

This study addresses the following research questions:

\begin{itemize}
    \item RQ1: How do ensemble methods (Random Forest, XGBoost) compare for air quality index category prediction?
    \item RQ2: Which text classification method (Logistic Regression, SVM) performs better for climate policy sentiment classification?
    \item RQ3: What features are most predictive of air quality categories?
    \item RQ4: How can model interpretability techniques enhance environmental decision-making?
\end{itemize}

The rest of this paper is structured as follows: Section II looks at what others have done with air quality prediction and climate text analysis. Section III walks through our methodology using CRISP-DM, including how we prepared the data. Section IV covers how we evaluated the models and what results we got. Section V discusses what our findings mean for real environmental applications. Section VI wraps up with limitations and ideas for future work.

\section{Related Work}

We reviewed existing work on air quality prediction, climate text analysis, and explainable AI. This helped us understand what's been done before and where our study could contribute something new.

\subsection{Air Quality Prediction}

Researchers have tried many machine learning techniques for air quality prediction. Zhan et al. \cite{zhan2017deep} found that deep learning works well for capturing time-based patterns, beating older time series methods. But they mostly looked at predicting exact AQI numbers (regression), while we're classifying into categories like "Good" or "Unhealthy"—which is more useful for issuing public warnings and meeting regulatory requirements.

Breiman's Random Forests \cite{breiman2001random} handle complex, non-linear patterns well, which is important for environmental data where relationships aren't always straightforward. They also naturally show which features matter most through Gini impurity scores, helping identify the pollutants that really drive air quality. The downside is they're sensitive to how you set their parameters, so we spent time tuning them carefully.

XGBoost, developed by Chen and Guestrin \cite{chen2016xgboost}, adds regularization to gradient boosting and runs efficiently. It tends to do well on environmental problems because it learns from mistakes, focusing more on the hard-to-predict cases. We decided to test both Random Forest (bagging) and XGBoost (boosting) to see which approach works better for our air quality problem.

Feng et al. \cite{feng2019air} conducted a comprehensive comparison of machine learning methods for air quality prediction, finding that ensemble methods consistently outperform individual models. Their work validates our algorithm selection but does not incorporate SHAP interpretability, which we add to provide actionable insights for environmental monitoring.

\subsection{Climate Text Sentiment Analysis}

As climate change dominates policy discussions, analyzing sentiment in environmental texts has become more important. Pang and Lee's review \cite{pang2008opinion} showed that supervised learning beats simple dictionary-based methods when you have labeled examples. Their work laid the groundwork for sentiment analysis, but they didn't specifically look at climate policy documents like we do.

Liu's work on Aspect-Based Sentiment Analysis \cite{liu2012sentiment} helps capture mixed feelings within a single document. While his research is more theoretical, we applied similar ideas using simpler, faster classifiers that could actually be deployed in real monitoring systems.

Support Vector Machines, introduced by Cortes and Vapnik \cite{cortes1995svm}, work well when you have many features. Joachims \cite{joachims1998text} later showed they're particularly good for text, with linear SVMs being both fast and accurate. These papers convinced us that SVMs were worth trying for our text classification task.

Zhang et al. \cite{zhang2018deep} compared old and new methods for sentiment analysis, finding that deep learning wins with huge datasets but simpler models are easier to understand. Since we wanted stakeholders to trust our results, we prioritized interpretability and used LIME to explain what the models were doing.

\subsection{Model Interpretability in Environmental Applications}

SHAP, developed by Lundberg and Lee \cite{lundberg2017shap}, uses game theory (Shapley values) to explain which features drive each prediction. It's consistent and accurate, though it can get slow with big datasets. We used TreeSHAP, a faster version designed for tree models, which kept our air quality analysis running smoothly.

LIME, from Ribeiro et al. \cite{ribeiro2016lime}, works by tweaking inputs and seeing how predictions change. It works with any model, which makes it perfect for text problems where you want to know which words matter. We used it to show exactly which terms in climate documents were driving sentiment predictions.

Molnar \cite{molnar2020interpretable} synthesized the state of the art in Explainable AI and provided an in-depth comparison of global and local explanation methods. His work provided a framework to identify LIME as the most appropriate method for word-level explanations in text classification and SHAP for feature importance explanations in structured data prediction.

\subsection{Research Gap}

Most previous studies cared more about accuracy than explaining their results. Also, researchers usually looked at either air quality OR text sentiment, not both together. We wanted to bridge this gap by analyzing both types of data in one project, using CRISP-DM to keep things organized, and applying both SHAP and LIME so our results would actually be useful for making environmental decisions.

\section{Data Mining Methodology}

This study follows the Cross-Industry Standard Process for Data Mining (CRISP-DM) framework \cite{chapman2000crisp}, encompassing six phases: business understanding, data understanding, data preparation, modeling, evaluation, and deployment.

\subsection{Business Understanding}

We focused on two problems that matter for environmental management:

\textbf{Air Quality Monitoring}: If we can automatically predict air quality categories, agencies can react faster to pollution spikes and spot long-term trends. This helps protect public health and ensures regulations are being followed.

\textbf{Climate Policy Analysis}: Understanding whether policy documents have positive, negative, or neutral sentiment helps policymakers see how their initiatives are being received. This could guide them in adjusting policies or improving how they communicate with the public.

\subsection{Data Understanding}

\subsubsection{Air Quality Dataset (Structured)}
The air quality dataset contains 15,000 hourly measurements with 20 attributes per record: timestamp, country, city, station type, temperature, humidity, wind speed, precipitation, PM2.5, PM10, NO2, CO, O3, SO2, AQI, AQI category, hour, day of week, month, and season. The target variable is AQI category, classified into six classes: Good, Moderate, Unhealthy for Sensitive Groups, Unhealthy, Very Unhealthy, and Hazardous. The class distribution shows moderate imbalance, with the majority of records classified as Moderate (approximately 62\%), followed by Unhealthy for Sensitive Groups (35\%), with smaller proportions for other categories.

\subsubsection{Climate Policy Text Dataset (Unstructured)}
The climate policy text dataset contains 9,500 documents with attributes including document ID, publication date, text content, sentiment, source type, urgency, impact score, and word count. The target variable is sentiment, classified as Positive, Negative, or Neutral. The class distribution is relatively balanced, with approximately 35\% Positive, 35\% Negative, and 30\% Neutral sentiment.

\begin{table}[h]
\centering
\caption{Dataset Characteristics}
\label{tab:datasets}
\begin{tabular}{lcc}
\toprule
\textbf{Attribute} & \textbf{Air Quality} & \textbf{Climate Text} \\
\midrule
Instances & 15,000 & 9,500 \\
Features & 20 & Variable (text) \\
Data Type & Numeric/Categorical & Text \\
Target & 6-class AQI category & 3-class sentiment \\
Class Balance & Imbalanced (62\% Moderate) & Balanced (35\% each) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Preparation}

\subsubsection{Air Quality Preprocessing Pipeline}
We cleaned the air quality data in several steps. About 2\% of values were missing, so we filled them using the median for each numeric column. We also found roughly 2\% of records were outliers using the IQR method and removed those. Then we created some new features that might help: a PM ratio (PM2.5 divided by PM10), a pollution index averaging the main pollutants, and binary flags for weekends and rush hours. Finally, we encoded the categorical target so the classification algorithms could work with it.

\subsubsection{Text Preprocessing Pipeline}
For the text data, we went through six preprocessing steps. First, we converted everything to lowercase. Then we stripped out HTML tags and URLs using regex. Next, we removed punctuation, special characters, and numbers. We used NLTK to split text into words (tokenization), removed common stopwords, and finally applied lemmatization to reduce the vocabulary size.

When converting text to numbers with TF-IDF, we set the maximum features to 5,000—enough to capture important terms without creating an unmanageable feature space. We included both single words and two-word phrases (bigrams) to catch terms like "climate change." We ignored words that appeared in fewer than 2 documents (too rare) or more than 95\% of documents (too common to be useful).

\textbf{Sampling Strategy}: To preserve class distributions and minimize computational cost, the full datasets were used with stratified train-test splitting. Stratification ensures proportionate representation of all classes in both training and testing sets.

\subsection{Modeling}

Four algorithms were implemented with each member responsible for two distinct models:

\textbf{Member 1 - Air Quality Prediction}: Random Forest and XGBoost

\textbf{Member 2 - Climate Text Classification}: Logistic Regression and SVM

\textbf{Model Specifications and Parameterization}:

\textbf{Random Forest}: We used 200 trees, which seemed like a good balance between stability and speed. We let trees grow to depth 20 so they could capture complex patterns without going too deep. To prevent overfitting, we required at least 5 samples to split a node and at least 2 samples per leaf. Since our classes were imbalanced, we used balanced class weights. Each tree only considered a random subset of features (sqrt of total).

\textbf{XGBoost}: We set the learning rate to 0.1—not too fast, not too slow. Trees were limited to depth 7, which is shallower than Random Forest because boosting already adds complexity through multiple rounds. We used 200 estimators and added L2 regularization (lambda=1.0) to keep things under control. We also set subsample and colsample\_bytree to 0.8 each for extra regularization.

\textbf{Logistic Regression}: We set C to 1.0, which seemed like a reasonable default. We allowed up to 1000 iterations to make sure it converged, and used the 'lbfgs' solver which worked well for our problem. Balanced class weights helped with the slight imbalance we had.

\textbf{Support Vector Machine}: We also used C=1.0 for SVM, balancing between a wide margin and correct classifications. We chose a linear kernel because our text features were already high-dimensional and sparse—a linear boundary was enough and much faster to compute. We enabled probability estimates so we could use LIME later.

\subsection{Model Interpretability Implementation}

\textbf{SHAP for Air Quality}: TreeSHAP efficiently computes Shapley values for tree ensembles. Provides both global feature importance (mean absolute SHAP values) and local explanations (individual prediction contributions).

\textbf{LIME for Text}: Perturbs input by randomly removing words, observes prediction changes, and fits a local linear model. Explains which words drive individual sentiment predictions for climate policy documents.

\section{Evaluation}

\subsection{Performance Metrics Selection}

We used several metrics to get a complete picture of how well our models performed:

\textbf{Accuracy}: Shows overall correctness, though it can be misleading when classes are imbalanced. We included it for comparison with other studies.

\textbf{Precision}: Tells us what fraction of positive predictions were actually correct. This matters for air quality alerts—we don't want too many false alarms.

\textbf{Recall (Sensitivity)}: Measures how many of the actual positive cases we caught. Important for catching pollution events and negative sentiment.

\textbf{F1-Score}: Combines precision and recall into one number. We used this as our main metric since both false positives and false negatives matter, and our classes were imbalanced.

\textbf{Cohen's Kappa}: Adjusts for chance agreement, which is helpful with imbalanced data. Scores above 0.6 mean the model is doing substantially better than random.

\textbf{Matthews Correlation Coefficient}: Another balanced metric that works well for multi-class problems.

\subsection{Sampling and Validation Strategy}

\textbf{Train-Test Split}: We split the data 75-25 for training and testing. We used stratified sampling so each subset had roughly the same class distribution as the original data.

\textbf{Cross-Validation}: When tuning hyperparameters, we used 5-fold cross-validation with stratification. This helped make sure our results weren't just lucky—we tested on multiple different splits, and each fold kept the original class balance.

\subsection{Results}

\subsubsection{Member 1: Air Quality Classification}

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{student1_model_comparison.png}
\caption{Member 1: Performance comparison showing Random Forest and XGBoost results}
\label{fig:member1_comparison}
\end{figure}

\begin{table}[h]
\centering
\caption{Member 1: Air Quality Classification Results}
\label{tab:member1_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Kappa} \\
\midrule
Random Forest & 0.987 & 0.987 & 0.987 & 0.987 & 0.972 \\
XGBoost & 0.986 & 0.986 & 0.986 & 0.986 & 0.970 \\
\bottomrule
\end{tabular}
\end{table}

Random Forest came out slightly ahead of XGBoost, though the difference was tiny. Random Forest hit 98.7\% accuracy compared to XGBoost's 98.6\%, which is really strong performance for a six-class problem. Both models had Cohen's Kappa scores above 0.97, meaning they're doing much better than random guessing and actually learning useful patterns from the data.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{rf_confusion_matrix.png}
\caption{Random Forest Confusion Matrix for Air Quality Classification}
\label{fig:rf_cm}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{xgb_confusion_matrix.png}
\caption{XGBoost Confusion Matrix for Air Quality Classification}
\label{fig:xgb_cm}
\end{figure}

\textbf{SHAP Feature Insights}:
\begin{enumerate}
    \item \textbf{PM2.5}: This came out as the top predictor. When PM2.5 goes up, air quality tends to get worse, which makes sense.
    \item \textbf{PM10}: Second most important. It's strongly tied to AQI categories.
    \item \textbf{NO2}: Nitrogen dioxide also plays a big role in predictions.
    \item \textbf{Temperature}: Weather matters too—temperature affects how pollutants behave in the atmosphere.
    \item \textbf{Hour and Rush Hour}: Time of day matters, probably because traffic patterns change throughout the day.
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{shap_rf_summary.png}
\caption{SHAP Summary Plot: Feature importance for Random Forest air quality model}
\label{fig:shap_rf}
\end{figure}

\subsubsection{Member 2: Climate Text Sentiment Classification}

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{student2_model_comparison.png}
\caption{Member 2: Performance comparison for Logistic Regression and SVM}
\label{fig:member2_comparison}
\end{figure}

\begin{table}[h]
\centering
\caption{Member 2: Climate Text Sentiment Classification Results}
\label{tab:member2_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} & \textbf{Kappa} \\
\midrule
Logistic Regression & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\
SVM & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

Both Logistic Regression and SVM got 100\% accuracy on our test set, which was surprising. We think this happened because our synthetic data has very clear boundaries between classes, and TF-IDF did a good job finding the vocabulary differences. In real policy documents with more ambiguous language, we'd expect lower (but still decent) performance.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{lr_confusion_matrix.png}
\caption{Logistic Regression Confusion Matrix for Sentiment Classification}
\label{fig:lr_cm}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{svm_confusion_matrix.png}
\caption{SVM Confusion Matrix for Sentiment Classification}
\label{fig:svm_cm}
\end{figure}

\textbf{LIME Text Insights}: When we looked at LIME explanations, we found that words like "effective", "successful", "improvement", "reduction", and "progress" pushed documents toward positive sentiment. Negative documents tended to have words like "failure", "concern", "threat", "decline", and "crisis". Neutral documents used more factual, policy-style language. These patterns make sense and help us trust what the models are doing.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{lime_explanation_sample1.png}
\caption{LIME Explanation: Sample negative sentiment classification}
\label{fig:lime1}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{lime_explanation_sample2.png}
\caption{LIME Explanation: Sample neutral sentiment classification}
\label{fig:lime2}
\end{figure}

\subsection{Cross-Dataset Comparison}

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{complete_dashboard.png}
\caption{Comprehensive comparison across all models and datasets}
\label{fig:complete_comparison}
\end{figure}

\begin{table*}[t]
\centering
\caption{Consolidated Results Across All Datasets and Models}
\label{tab:all_results}
\begin{tabular}{llcccccc}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Cohen's Kappa} & \textbf{MCC} \\
\midrule
Air Quality & Random Forest & 0.987 & 0.987 & 0.987 & 0.987 & 0.972 & 0.972 \\
Air Quality & XGBoost & 0.986 & 0.986 & 0.986 & 0.986 & 0.970 & 0.970 \\
Climate Text & Logistic Regression & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\
Climate Text & SVM & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Implications of Results}

\textbf{Air Quality Prediction}: Both ensemble methods performed really well—Random Forest at 98.7\% and XGBoost at 98.6\%. The high Kappa scores (0.972 and 0.970) show they're learning real patterns, not just memorizing. SHAP helped us see that PM2.5, PM10, and NO2 are the pollutants that matter most, which makes sense from an environmental science perspective.

\textbf{Climate Text Sentiment}: Getting 100\% accuracy with both Logistic Regression and SVM was unexpected. TF-IDF plus linear classifiers clearly worked well for our data, though we suspect the synthetic nature helped. Still, this validates the approach—even if real-world performance drops, it should remain competitive. LIME showed us exactly which words drive predictions, which helps build trust with potential users.

\textbf{Parameterization Impact}: Our hyperparameter choices (Random Forest: depth 20, 200 trees; XGBoost: depth 7, learning rate 0.1) seemed to work well. Both models performed consistently on training and test data, suggesting we found a good balance between fitting the data and generalizing.

\section{Discussion}

\subsection{Addressing Research Questions}

\textbf{RQ1}: Random Forest beat XGBoost by a tiny margin (F1: 0.987 vs 0.986), but honestly both did great. Either one would work well in a real monitoring system.

\textbf{RQ2}: Logistic Regression and SVM both got perfect scores, which was interesting. The linear models plus TF-IDF created clear boundaries between classes in our synthetic data. In practice, we'd expect some drop but still good results.

\textbf{RQ3}: SHAP told us that PM2.5, PM10, NO2, temperature, and time-based features (hour, rush hour) are the strongest predictors. This gives agencies clear targets for where to focus monitoring efforts.

\textbf{RQ4}: SHAP and LIME made a big difference in making our models understandable. For air quality, we can show which pollutants matter most. For text, we can point to specific words driving sentiment. This helps people trust and actually use the models.

\subsection{Environmental Applications}

\textbf{Automated Air Quality Monitoring}: These models could run continuously, automatically categorizing new sensor readings and sending alerts when air quality gets bad. This would help protect public health.

\textbf{Climate Policy Analysis}: Sentiment classifiers could monitor policy documents in real-time, giving policymakers a sense of how their initiatives are being received. They could then adjust their messaging or policies accordingly.

\textbf{Resource Optimization}: Since we know PM2.5, PM10, and NO2 matter most, agencies could focus monitoring efforts on these pollutants rather than trying to track everything equally. This would be more efficient.

\subsection{Limitations}

Our study has some limitations worth noting. Since we used synthetic data, real environmental measurements might have different patterns, more noise, or different class distributions. The 100\% text classification accuracy probably won't hold up with real policy documents that have more ambiguous language. We also didn't do exhaustive hyperparameter tuning due to time constraints, and we skipped deep learning entirely. Another limitation is that we didn't account for how air quality changes over time or how nearby locations might be correlated. Finally, if these models were actually deployed, we'd need to keep checking for concept drift as conditions change.

\section{Conclusions and Future Work}

This study compared machine learning methods for environmental impact analysis following CRISP-DM methodology across structured air quality data and unstructured climate policy text.

\textbf{Key Findings}:
\begin{enumerate}
    \item Both Random Forest and XGBoost worked really well for air quality (98.7\% and 98.6\%), showing that ensemble methods are a solid choice for environmental problems.
    \item Logistic Regression and SVM both got perfect scores on sentiment classification, proving that TF-IDF plus simple linear models can work great for policy text.
    \item The interpretability tools gave us useful insights: PM2.5, PM10, and NO2 drive air quality predictions, while specific word choices determine sentiment.
    \item Following CRISP-DM kept us organized and helped make sure our technical work actually addressed the environmental problems we cared about.
\end{enumerate}

\textbf{Answers to Research Questions}: Ensemble methods clearly work well for air quality—both Random Forest and XGBoost performed excellently. For text sentiment, TF-IDF with linear classifiers is effective and interpretable. The interpretability tools (SHAP and LIME) really do help translate model outputs into actionable insights.

\textbf{Future Work}: We'd like to try deep learning next—maybe LSTMs for capturing time patterns in air quality or BERT for better text understanding, though we'd lose some interpretability. Modeling how air quality changes over time could help predict pollution events earlier. Real-time deployment would let agencies respond immediately. Adding satellite data and more sensor networks could improve predictions. Finally, we'd want to test whether interventions based on these models actually help in practice.

\section*{Contribution Summary}

\textbf{Gitte Swapna (Member 1)}: Air quality data analytics, feature engineering, Random Forest and XGBoost implementation, SHAP interpretability analysis, structured data preprocessing pipeline.

\textbf{Hemasree Challa (Member 2)}: Climate policy text analysis, TF-IDF vectorization, Logistic Regression and SVM implementation, LIME interpretability analysis, text preprocessing pipeline.

\textbf{Joint Contributions}: Literature review, methodology design, cross-dataset comparison, report writing, presentation preparation.

\begin{thebibliography}{00}
\bibitem{chapman2000crisp} P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, and R. Wirth, ``CRISP-DM 1.0: Step-by-step data mining guide,'' SPSS Inc., 2000.

\bibitem{zhan2017deep} Y. Zhan, Y. Luo, X. Deng, M. Zhang, K. Grieneisen, B. Di, and F. Zhang, ``Deep learning approach for predicting daily PM2.5 concentrations,'' Environmental Science \& Technology, vol. 51, no. 12, pp. 6999--7005, 2017.

\bibitem{breiman2001random} L. Breiman, ``Random forests,'' Machine Learning, vol. 45, no. 1, pp. 5--32, 2001.

\bibitem{chen2016xgboost} T. Chen and C. Guestrin, ``XGBoost: A scalable tree boosting system,'' ACM SIGKDD, pp. 785--794, 2016.

\bibitem{feng2019air} X. Feng, Q. Li, Y. Zhu, J. Hou, L. Jin, and J. Wang, ``Artificial neural networks forecasting of PM2.5 pollution using air mass trajectory based geographic model and wavelet transformation,'' Atmospheric Environment, vol. 107, pp. 118--128, 2015.

\bibitem{pang2008opinion} B. Pang and L. Lee, ``Opinion mining and sentiment analysis,'' Foundations and Trends in Information Retrieval, vol. 2, no. 1-2, pp. 1--135, 2008.

\bibitem{liu2012sentiment} B. Liu, ``Sentiment analysis and opinion mining,'' Synthesis Lectures on Human Language Technologies, vol. 5, no. 1, pp. 1--167, 2012.

\bibitem{cortes1995svm} C. Cortes and V. Vapnik, ``Support-vector networks,'' Machine Learning, vol. 20, no. 3, pp. 273--297, 1995.

\bibitem{joachims1998text} T. Joachims, ``Text categorization with support vector machines: Learning with many relevant features,'' European Conference on Machine Learning, pp. 137--142, 1998.

\bibitem{zhang2018deep} L. Zhang, S. Wang, and B. Liu, ``Deep learning for sentiment analysis: A survey,'' WIREs Data Mining and Knowledge Discovery, vol. 8, no. 4, e1253, 2018.

\bibitem{lundberg2017shap} S. M. Lundberg and S. I. Lee, ``A unified approach to interpreting model predictions,'' NeurIPS, vol. 30, 2017.

\bibitem{ribeiro2016lime} M. T. Ribeiro, S. Singh, and C. Guestrin, ``Why should I trust you?: Explaining the predictions of any classifier,'' ACM SIGKDD, pp. 1135--1144, 2016.

\bibitem{molnar2020interpretable} C. Molnar, ``Interpretable machine learning: A guide for making black box models explainable,'' 2020.

\end{thebibliography}

\end{document}

